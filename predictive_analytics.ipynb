{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9033c393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Task 3: Predictive Analytics for Resource Allocation\n",
    "# ## Using Machine Learning to Predict Issue Priority\n",
    "# \n",
    "# **Dataset:** Breast Cancer Wisconsin (Diagnostic) Dataset\n",
    "# **Model:** Random Forest Classifier\n",
    "# **Objective:** Predict issue priority (High/Medium/Low) for resource allocation\n",
    "\n",
    "# %%\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, f1_score, classification_report, \n",
    "                           confusion_matrix, precision_recall_curve, roc_curve, auc)\n",
    "from sklearn.inspection import permutation_importance\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# %%\n",
    "# Load and explore the dataset\n",
    "print(\"Loading Breast Cancer Dataset...\")\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Target distribution: {pd.Series(y).value_counts()}\")\n",
    "print(f\"Feature names: {data.feature_names}\")\n",
    "\n",
    "# %%\n",
    "# Convert binary classification to multi-class for priority prediction\n",
    "# Simulating software issue priorities: 0=Low, 1=Medium, 2=High\n",
    "np.random.seed(42)\n",
    "y_priority = np.where(y == 1, 2, 0)  # Convert 1s to High(2), 0s to Low(0)\n",
    "\n",
    "# Introduce medium priority samples by adding noise to some high priority cases\n",
    "medium_mask = (y_priority == 2) & (np.random.random(len(y_priority)) < 0.3)\n",
    "y_priority[medium_mask] = 1\n",
    "\n",
    "print(\"Target Priority Distribution:\")\n",
    "priority_counts = pd.Series(y_priority).value_counts().sort_index()\n",
    "print(priority_counts)\n",
    "print(f\"Low: {priority_counts[0]}, Medium: {priority_counts[1]}, High: {priority_counts[2]}\")\n",
    "\n",
    "# %%\n",
    "# Data preprocessing\n",
    "print(\"Preprocessing data...\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_priority, test_size=0.3, random_state=42, stratify=y_priority\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test set shape: {X_test_scaled.shape}\")\n",
    "\n",
    "# %%\n",
    "# Train Random Forest model\n",
    "print(\"Training Random Forest Classifier...\")\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_proba = model.predict_proba(X_test_scaled)\n",
    "\n",
    "# %%\n",
    "# Model evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-Score (weighted): {f1:.4f}\")\n",
    "\n",
    "# Cross-validation scores\n",
    "cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-validation Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# %%\n",
    "# Detailed classification report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred, \n",
    "                          target_names=['Low Priority', 'Medium Priority', 'High Priority']))\n",
    "\n",
    "# %%\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Low', 'Medium', 'High'],\n",
    "            yticklabels=['Low', 'Medium', 'High'])\n",
    "plt.title('Confusion Matrix - Issue Priority Prediction')\n",
    "plt.xlabel('Predicted Priority')\n",
    "plt.ylabel('Actual Priority')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Feature Importance Analysis\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': data.feature_names,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=feature_importance.head(15), x='importance', y='feature')\n",
    "plt.title('Top 15 Most Important Features for Priority Prediction')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "# %%\n",
    "# Precision-Recall Curve for each class\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, priority in enumerate(['Low', 'Medium', 'High']):\n",
    "    precision, recall, _ = precision_recall_curve(\n",
    "        (y_test == i), y_pred_proba[:, i]\n",
    "    )\n",
    "    plt.plot(recall, precision, lw=2, \n",
    "             label=f'{priority} Priority (AP = {auc(recall, precision):.2f})')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve by Priority Class')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Model Insights and Business Impact\n",
    "print(\"=\"*60)\n",
    "print(\"BUSINESS IMPACT ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate potential time savings\n",
    "total_issues = len(y_test)\n",
    "correct_predictions = np.sum(y_test == y_pred)\n",
    "\n",
    "low_issues = np.sum(y_test == 0)\n",
    "medium_issues = np.sum(y_test == 1)\n",
    "high_issues = np.sum(y_test == 2)\n",
    "\n",
    "print(f\"Total issues in test set: {total_issues}\")\n",
    "print(f\"Correctly prioritized: {correct_predictions} ({accuracy*100:.1f}%)\")\n",
    "print(f\"\\nPriority Distribution in Test Set:\")\n",
    "print(f\"  Low Priority: {low_issues} issues\")\n",
    "print(f\"  Medium Priority: {medium_issues} issues\")\n",
    "print(f\"  High Priority: {high_issues} issues\")\n",
    "\n",
    "# Estimated time savings (in hours)\n",
    "time_per_issue = [2, 8, 16]  # Hours per issue type: Low, Medium, High\n",
    "total_time_without_ai = sum([\n",
    "    low_issues * time_per_issue[0],\n",
    "    medium_issues * time_per_issue[1],\n",
    "    high_issues * time_per_issue[2]\n",
    "])\n",
    "\n",
    "# With AI, high priority issues get addressed faster\n",
    "time_savings = high_issues * (time_per_issue[2] - time_per_issue[1]) * 0.3  # 30% efficiency gain\n",
    "\n",
    "print(f\"\\nEstimated Resource Allocation Impact:\")\n",
    "print(f\"Total processing time without AI: {total_time_without_ai} hours\")\n",
    "print(f\"Estimated time savings with AI prioritization: {time_savings:.1f} hours\")\n",
    "print(f\"Efficiency improvement: {(time_savings/total_time_without_ai)*100:.1f}%\")\n",
    "\n",
    "# %%\n",
    "# Save performance metrics for report\n",
    "performance_metrics = {\n",
    "    'accuracy': accuracy,\n",
    "    'f1_score': f1,\n",
    "    'cv_accuracy_mean': cv_scores.mean(),\n",
    "    'cv_accuracy_std': cv_scores.std(),\n",
    "    'feature_importance': feature_importance,\n",
    "    'confusion_matrix': cm\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"✅ Data preprocessing completed\")\n",
    "print(\"✅ Random Forest model trained\")\n",
    "print(\"✅ Performance metrics calculated\")\n",
    "print(\"✅ Feature importance analyzed\")\n",
    "print(\"✅ Business impact assessed\")\n",
    "\n",
    "# Save important features to CSV\n",
    "feature_importance.head(10).to_csv('top_features.csv', index=False)\n",
    "print(\"✅ Top features saved to 'top_features.csv'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
